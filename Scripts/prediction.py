# -*- coding: utf-8 -*-
"""GDB13_28__HOMO_LUMO0703.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1R6D2h8qs_12LnApngmK4CyqN82WjJiE2

#Installation Packages
"""

import warnings
warnings.filterwarnings("ignore")

from rdkit.Chem import AllChem
from rdkit import Chem
from rdkit.Chem import Descriptors
from rdkit.ML.Descriptors import MoleculeDescriptors
from rdkit.Chem import Draw
#from rdkit.Chem.Draw import IPythonConsole

import pandas as pd
import numpy as np

from sklearn.model_selection import cross_val_score,train_test_split
from sklearn.metrics import r2_score,mean_absolute_error, mean_squared_error
from sklearn.preprocessing import StandardScaler

#from matplotlib import pyplot as plt
#import matplotlib.patches as mpatches
#import seaborn as sn

import pandas as pd
import numpy as np
from rdkit import Chem
from rdkit.ML.Descriptors import MoleculeDescriptors
from rdkit.Chem import Descriptors
from sklearn.model_selection import train_test_split

# Declare folder name as a variable
fd = "GDB13_28_all"

# Load training, validation, and test datasets
train_file = f"{fd}/train.csv"
val_file = f"{fd}/valid.csv"
test_file = f"{fd}/test.csv"

df_train = pd.read_csv(train_file)
df_val = pd.read_csv(val_file)
df_test = pd.read_csv(test_file)

# Combine datasets for descriptor generation and feature selection
df_all = pd.concat([df_train, df_val, df_test], ignore_index=True)
new_data = df_all[['smiles', 'homolumogap']]

# Generate RDKit descriptors
def RDkit_descriptors(smiles):
    mols = [Chem.MolFromSmiles(i) for i in smiles]
    calc = MoleculeDescriptors.MolecularDescriptorCalculator([x[0] for x in Descriptors._descList])
    desc_names = calc.GetDescriptorNames()

    Mol_descriptors = []
    for mol in mols:
        mol = Chem.AddHs(mol)  # Add hydrogens
        descriptors = calc.CalcDescriptors(mol)  # Calculate descriptors
        Mol_descriptors.append(descriptors)

    return Mol_descriptors, desc_names

Mol_descriptors, desc_names = RDkit_descriptors(new_data['smiles'])

# Create a DataFrame for descriptors
df_descriptor = pd.DataFrame(Mol_descriptors, columns=desc_names)

# Remove correlated features across the whole dataset
def remove_correlated_features(descriptors):
    correlated_matrix = descriptors.corr().abs()
    upper_triangle = correlated_matrix.where(np.triu(np.ones(correlated_matrix.shape), k=1).astype(bool))
    to_drop = [column for column in upper_triangle.columns if any(upper_triangle[column] >= 0.90)]
    print(f"Removing correlated features: {to_drop}")
    return descriptors.drop(columns=to_drop, axis=1)

df_descriptor_filtered = remove_correlated_features(df_descriptor)

# Re-split into respective datasets
X_all = df_descriptor_filtered
y_all = new_data['homolumogap']

# Retrieve indices corresponding to original splits
n_train = len(df_train)
n_val = len(df_val)

X_train, y_train = X_all.iloc[:n_train], y_all.iloc[:n_train]
X_val, y_val = X_all.iloc[n_train:n_train+n_val], y_all.iloc[n_train:n_train+n_val]
X_test, y_test = X_all.iloc[n_train+n_val:], y_all.iloc[n_train+n_val:]

# Verify dataset shapes
print(f"X_train: {X_train.shape}, y_train: {y_train.shape}")
print(f"X_val: {X_val.shape}, y_val: {y_val.shape}")
print(f"X_test: {X_test.shape}, y_test: {y_test.shape}")

"""# Catboost Regressor"""

from catboost import CatBoostRegressor
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import train_test_split

#import pandas as pd
import matplotlib.pyplot as plt
#from catboost import CatBoostRegressor
from sklearn.model_selection import train_test_split

# Initialize the CatBoostRegressor with the best hyperparameters
final_model_CatB = CatBoostRegressor(
    iterations=4000,
    depth=10,
    learning_rate=0.08003866739082473,
    l2_leaf_reg=1.2219300331335978,
    bagging_temperature=0.9026337873010564,
    random_seed=42,
    verbose=100,  # Log progress every 100 iterations
    eval_metric='MAE',  # Use MAE as evaluation metric
    loss_function='RMSE'  # Use RMSE as the loss function
)


# save the trained model weight
#final_model_CatB.save_model(f'{fd}/CatBoost_model.weights.h5')

#load saved model
final_model_CatB.load_model(f'{fd}/CatBoost_model.weights.h5')


from lightgbm import LGBMRegressor
import lightgbm as lgb
from sklearn.preprocessing import StandardScaler
import os

# Assuming X_train, X_val, X_test, y_train, y_val, y_test are already defined

# Scale the data
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_val_scaled = scaler.transform(X_val)
X_test_scaled = scaler.transform(X_test)

# Initialize the LightGBM model with MSE as loss
lgbm_model = LGBMRegressor(
    force_col_wise=True,
    learning_rate=0.03,
    num_leaves=125,
    max_depth=20,
    min_child_samples=15,
    subsample=1.0,
    colsample_bytree=0.7,
    n_estimators=4000,
    reg_alpha=0.1,
    reg_lambda=0.1,
    random_state=777,
    verbose=100,
    objective='regression',  # Default for regression (MSE loss)
)

#save trained lgbm model so that I can load it latter
#import pickle
#filename = f'{fd}/LGBM_model_weights.txt'
#pickle.dump(lgbm_model, open(filename, 'wb'))

#load saved model
import pickle
filename = f'{fd}/LGBM_model_weights.txt'
lgbm_model = pickle.load(open(filename, 'rb'))

import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Bidirectional, Dense
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.preprocessing import StandardScaler
import random as random

# Set random seed for reproducibility
seed_value = 42
np.random.seed(seed_value)
random.seed(seed_value)
tf.random.set_seed(seed_value)

# Scale the data
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_val_scaled = scaler.transform(X_val)
X_test_scaled = scaler.transform(X_test)

# Reshape the data for LSTM (3D input: samples, timesteps, features)
X_train_reshaped = np.reshape(X_train_scaled, (X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))
X_val_reshaped = np.reshape(X_val_scaled, (X_val_scaled.shape[0], 1, X_val_scaled.shape[1]))
X_test_reshaped = np.reshape(X_test_scaled, (X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))

# Define the BiLSTM model
Bilstm_model = Sequential([
    Bidirectional(LSTM(units=64, activation='relu', return_sequences=True), input_shape=(1, X_train.shape[1])),
    Bidirectional(LSTM(units=64, activation='relu', return_sequences=True)),
    Bidirectional(LSTM(units=64, activation='relu', return_sequences=True)),
    Bidirectional(LSTM(units=64, activation='relu', return_sequences=True)),
    Bidirectional(LSTM(units=64, activation='relu')),
    Dense(units=1)  # Output layer for regression
])

# Define a custom learning rate for Adam optimizer
from tensorflow.keras.optimizers import Adam
optimizer = Adam(learning_rate=0.001)  # Custom learning rate

# Compile the model with Mean Squared Error loss function
Bilstm_model.compile(optimizer=optimizer, loss='mean_squared_error')

# Early stopping callback
early_stopping = EarlyStopping(monitor='val_loss', patience=100, restore_best_weights=True)


#save the model weight
#Bilstm_model.save_weights(f'{fd}/Bi_LSTM_model.weights.h5')

#load the saved model
Bilstm_model.load_weights(f'{fd}/Bi_LSTM_model.weights.h5')


"""# Multilayer Perceptron (MLP)"""

from sklearn.neural_network import MLPRegressor
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import StandardScaler
import os

# Assuming X_train, X_val, X_test, y_train, y_val, y_test are already defined

# Scale the data (train, validation, and test)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_val_scaled = scaler.transform(X_val)
X_test_scaled = scaler.transform(X_test)

# Initialize the MLPRegressor
mlp1 = MLPRegressor(
    hidden_layer_sizes=(500, 100, 50),
    max_iter=1000,
    activation='relu',
    solver='adam',
    random_state=999,
    learning_rate='adaptive',
    learning_rate_init=0.001,
    warm_start=True
)

import pickle

# Save the entire model using pickle
#with open(f'{fd}/mlp1_model.pkl', 'wb') as file:
    #pickle.dump(mlp1, file)

#Load the saved model
with open(f'{fd}/mlp1_model.pkl', 'rb') as file:
    mlp1 = pickle.load(file)


###Ensemble_Model####
import numpy as np
import itertools
import pandas as pd
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# Generate predictions on the training, Validation and testing data
lgbm_train_preds = lgbm_model.predict(X_train_scaled)
lstm_train_preds = Bilstm_model.predict(X_train_reshaped).flatten()
mlp_train_preds = mlp1.predict(X_train_scaled)
catb_train_preds = final_model_CatB.predict(X_train)

lgbm_val_preds = lgbm_model.predict(X_val_scaled)
lstm_val_preds = Bilstm_model.predict(X_val_reshaped).flatten()
mlp_val_preds = mlp1.predict(X_val_scaled)
catb_val_preds = final_model_CatB.predict(X_val)

lgbm_test_preds = lgbm_model.predict(X_test_scaled)
lstm_test_preds = Bilstm_model.predict(X_test_reshaped).flatten()
mlp_test_preds = mlp1.predict(X_test_scaled)
catb_test_preds = final_model_CatB.predict(X_test)

# Define weights for each base model
lgbm_weight = 0.7
lstm_weight = 0.2
mlp_weight = 0.1
#cat_weight = 0.00

# Combine predictions with weights for training and validation
weighted_train_preds = (
    lgbm_weight * lgbm_train_preds +
    lstm_weight * lstm_train_preds +
    mlp_weight * mlp_train_preds
    #cat_weight * catb_train_preds
)

weighted_val_preds = (
    lgbm_weight * lgbm_val_preds +
    lstm_weight * lstm_val_preds +
    mlp_weight * mlp_val_preds
    #cat_weight * catb_val_preds
)

weighted_test_preds = (
    lgbm_weight * lgbm_test_preds +
    lstm_weight * lstm_test_preds +
    mlp_weight * mlp_test_preds
    #cat_weight * catb_test_preds
)

# Define a random state for reproducibility
random_state = 42

# Initialize the Gradient Boosting model with MSE as loss function
esm_model = GradientBoostingRegressor(
    n_estimators=4000,
    learning_rate=0.001,
    max_depth=10,
    loss='squared_error',  # Explicitly setting MSE as the loss function
    random_state=random_state  # Ensuring reproducibility
)

# Fit the model
#esm_model.fit(weighted_train_preds.reshape(-1, 1), y_train)
    
#load the esm model weight
with open(f'{fd}/esm_model.pkl', 'rb') as file:
    esm_model = pickle.load(file)

##Prediction data Preparation##

fdp = "Prediction"

#load the csv file
new_sample_data=pd.read_csv(f'{fdp}/unknownc5toc8.csv')
new_sample_data=new_sample_data[['smiles', 'homolumogap']]
new_sample_data
# generate the descriptor using RdKit

def RDkit_descriptors(smiles):
    mols = [Chem.MolFromSmiles(i) for i in smiles]
    calc = MoleculeDescriptors.MolecularDescriptorCalculator([x[0]
                                    for x in Descriptors._descList])
    desc_names = calc.GetDescriptorNames()

    Mol_descriptors =[]
    for mol in mols:
        # add hydrogens to molecules
        mol=Chem.AddHs(mol)
        # Calculate all the descriptors for each molecule
        descriptors = calc.CalcDescriptors(mol)
        Mol_descriptors.append(descriptors)
    return Mol_descriptors,desc_names

# Function call
Mol_descriptors,desc_names = RDkit_descriptors(new_sample_data['smiles'])
X_sample = pd.DataFrame(Mol_descriptors,columns=desc_names)

# X_sample should have same column of aev_test
X_sample = X_sample[X_all.columns]
#Relapce nan with 0 and -inf with the largest value
#X_sample = X_sample.fillna(0)
#X_sample = X_sample.replace(-np.inf, X_sample.replace(-np.inf, 0).max().max())
y_sample = new_sample_data.homolumogap
X_sample_scaled = scaler.transform(X_sample)

X_sample_reshaped = np.reshape(X_sample_scaled, (X_sample_scaled.shape[0], 1, X_sample_scaled.shape[1]))

lgbm_sample_preds = lgbm_model.predict(X_sample_scaled)
lstm_sample_preds = Bilstm_model.predict(X_sample_reshaped).flatten()
mlp_sample_preds = mlp1.predict(X_sample_scaled)

weighted_sample_preds = (lgbm_weight * lgbm_sample_preds) + (lstm_weight * lstm_sample_preds) + (mlp_weight * mlp_sample_preds)

pred_sample_esm = esm_model.predict(weighted_sample_preds.reshape(-1, 1))

mae_sample = mean_absolute_error(y_sample, pred_sample_esm)
mse_sample = mean_squared_error(y_sample, pred_sample_esm)
R2_sample = r2_score(y_sample, pred_sample_esm)
print("Mean Absolute Error:", mae_sample)
print("Squared Absolute Error:", mae_sample)
print('R^2:', R2_sample)

import pandas as pd
df_sample_predictions = pd.DataFrame({
    'Actual': y_sample,  # Actual values
    'Predicted': pred_sample_esm  # Predicted values
})

df_sample_predictions.to_csv(f'{fdp}/esm_sample_actual_vs_predicted.csv', index=False)
print("CSV files saved successfully!")
